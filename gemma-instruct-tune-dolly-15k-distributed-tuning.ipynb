{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"modelInstanceVersion","sourceId":85984,"databundleVersionId":9247149,"modelInstanceId":72244}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install -q -U keras-nlp\n! pip install -q -U \"keras>=3\"\n! pip install -q -U wandb","metadata":{"id":"_ZGHU975nyGn","execution":{"iopub.status.busy":"2024-08-31T12:08:59.081425Z","iopub.execute_input":"2024-08-31T12:08:59.082326Z","iopub.status.idle":"2024-08-31T12:09:56.382827Z","shell.execute_reply.started":"2024-08-31T12:08:59.082283Z","shell.execute_reply":"2024-08-31T12:09:56.381529Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import keras_nlp\nimport keras\nimport tensorflow as tf\nimport jax\nimport wandb\nimport json\nimport os","metadata":{"id":"-9jQCvBEosiC","execution":{"iopub.status.busy":"2024-08-31T12:10:19.812416Z","iopub.execute_input":"2024-08-31T12:10:19.813114Z","iopub.status.idle":"2024-08-31T12:10:32.444652Z","shell.execute_reply.started":"2024-08-31T12:10:19.813059Z","shell.execute_reply":"2024-08-31T12:10:32.443846Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# The Keras 3 distribution API is only implemented for the JAX backend for now\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.09\"\n\n# The Keras 3 distribution API is only implemented for the JAX backend for now\n# keras.mixed_precision.set_global_policy('mixed_bfloat16')\n\njax.devices()","metadata":{"id":"z05f-Bf_o_qL","execution":{"iopub.status.busy":"2024-08-31T12:10:33.926914Z","iopub.execute_input":"2024-08-31T12:10:33.927323Z","iopub.status.idle":"2024-08-31T12:10:33.932184Z","shell.execute_reply.started":"2024-08-31T12:10:33.927278Z","shell.execute_reply":"2024-08-31T12:10:33.931131Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# strategy = tf.distribute.MirroredStrategy()\n# print('DEVICES AVAILABLE: {}'.format(strategy.num_replicas_in_sync))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices()\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\nlayout_map[\"token_embedding/embeddings\"] = (model_dim, None)\n# Regex to match against the query, key and value matrices in the decoder\n# attention layers\nlayout_map[\"decoder_block.*attention.*(query|key|value).*kernel\"] = (\n    model_dim, None, None)\n\nlayout_map[\"decoder_block.*attention_output.*kernel\"] = (\n    model_dim, None, None)\nlayout_map[\"decoder_block.*ffw_gating.*kernel\"] = (None, model_dim)\nlayout_map[\"decoder_block.*ffw_linear.*kernel\"] = (model_dim, None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_parallel = keras.distribution.ModelParallel(\n    device_mesh, layout_map, batch_dim_name=\"batch\")\n\nkeras.distribution.set_distribution(model_parallel)\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")","metadata":{"id":"IJoVLRrT1ZSn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder_block_1 = gemma_lm.backbone.get_layer('decoder_block_1')\nprint(type(decoder_block_1))\nfor variable in decoder_block_1.weights:\n  print(f'{variable.path:<58}  {str(variable.shape):<16}  {str(variable.value.sharding.spec)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login()","metadata":{"id":"x6m-EZT0xUHE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 5e-5\nweight_decay = 0.01\nepochs = 1\nbatch_size = 8","metadata":{"id":"LVWzPs0Dxpzp","execution":{"iopub.status.busy":"2024-08-31T12:11:28.785805Z","iopub.execute_input":"2024-08-31T12:11:28.787220Z","iopub.status.idle":"2024-08-31T12:11:28.791256Z","shell.execute_reply.started":"2024-08-31T12:11:28.787176Z","shell.execute_reply":"2024-08-31T12:11:28.790309Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"gemma2_2b-instruct-tune\",\n           config={\n               \"architecture\": \"gemma 2\",\n               \"dataset\": \"databricks-dolly-15k\",\n               \"epochs\": epochs,\n               \"batch_size\": batch_size,\n               \"learning_rate\": learning_rate,\n               \"weight_decay\": weight_decay,\n               }\n           )","metadata":{"id":"yn09nnmgxqHd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! wget -O databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl","metadata":{"id":"241_awaksecl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"952eb7aa-a81e-4b76-c0e5-e309923c8f0a","execution":{"iopub.status.busy":"2024-08-31T12:11:53.442296Z","iopub.execute_input":"2024-08-31T12:11:53.442942Z","iopub.status.idle":"2024-08-31T12:11:57.304712Z","shell.execute_reply.started":"2024-08-31T12:11:53.442903Z","shell.execute_reply":"2024-08-31T12:11:57.303595Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"--2024-08-31 12:11:54--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\nResolving huggingface.co (huggingface.co)... 13.35.7.81, 13.35.7.5, 13.35.7.57, ...\nConnecting to huggingface.co (huggingface.co)|13.35.7.81|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1725365514&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNTM2NTUxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=OXLWBwp2xwMQ9eM8Mh4jsYVwXGLWwPKu7QdxdtrqeKvooAxls2Vjc%7EE822VK0qJSYfPmtKgCg8SHPogNbdgvuJITLh4aNvDteeObeHLNGujYZVjYmY4fZwOOqVYhV3%7E6Ak3pxEFEJkvRSBIggy0-qRZlIRWDOCJt%7ETTmwv3gujshXGr-DRaH7OgUWW756X4wNOZWkc0ulcuQXLlV249jv4Q9Tu4zbYAZ-vLpfqpsBt107QSTQwvqPmiIeMdg%7EPQFlwNRoQ0EtabtOAq-BhSJzX4eIuNwqIf5cEN9N3FeD-Y4cN0iceloGH%7EHscY5SZFfSbamITjRhil0PXauHy3Hvw__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n--2024-08-31 12:11:54--  https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8''databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1725365514&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNTM2NTUxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=OXLWBwp2xwMQ9eM8Mh4jsYVwXGLWwPKu7QdxdtrqeKvooAxls2Vjc~E822VK0qJSYfPmtKgCg8SHPogNbdgvuJITLh4aNvDteeObeHLNGujYZVjYmY4fZwOOqVYhV3~6Ak3pxEFEJkvRSBIggy0-qRZlIRWDOCJt~TTmwv3gujshXGr-DRaH7OgUWW756X4wNOZWkc0ulcuQXLlV249jv4Q9Tu4zbYAZ-vLpfqpsBt107QSTQwvqPmiIeMdg~PQFlwNRoQ0EtabtOAq-BhSJzX4eIuNwqIf5cEN9N3FeD-Y4cN0iceloGH~HscY5SZFfSbamITjRhil0PXauHy3Hvw__&Key-Pair-Id=K3ESJI6DHPFC7\nResolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.35.7.14, 13.35.7.113, 13.35.7.99, ...\nConnecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.35.7.14|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13085339 (12M) [text/plain]\nSaving to: 'databricks-dolly-15k.jsonl'\n\ndatabricks-dolly-15 100%[===================>]  12.48M  7.17MB/s    in 1.7s    \n\n2024-08-31 12:11:57 (7.17 MB/s) - 'databricks-dolly-15k.jsonl' saved [13085339/13085339]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"data = []\nwith open(\"databricks-dolly-15k.jsonl\") as file:\n    for line in file:\n        features = json.loads(line)\n        # Filter out examples with context, to keep it simple.\n        if features[\"context\"]:\n            continue\n        # Format the entire example as a single string.\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        data.append(template.format(**features))\n\n# Only use 1000 training examples, to keep it fast.\ndata = data[:1000]","metadata":{"id":"3JfMOlhbpoc3","execution":{"iopub.status.busy":"2024-08-31T12:12:02.337599Z","iopub.execute_input":"2024-08-31T12:12:02.338002Z","iopub.status.idle":"2024-08-31T12:12:02.487380Z","shell.execute_reply.started":"2024-08-31T12:12:02.337963Z","shell.execute_reply":"2024-08-31T12:12:02.486159Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"What should I do on a trip to Europe?\",\n    response=\"\",\n)\n\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n\ngemma_lm.compile(sampler=sampler)\n\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"id":"rL--NTcLrkvs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e0522c6-d9da-4c8f-aa8f-51c0637111b5","execution":{"iopub.status.busy":"2024-08-31T11:53:01.355866Z","iopub.execute_input":"2024-08-31T11:53:01.356281Z","iopub.status.idle":"2024-08-31T11:53:48.944541Z","shell.execute_reply.started":"2024-08-31T11:53:01.356241Z","shell.execute_reply":"2024-08-31T11:53:48.943627Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1725105195.432034      36 service.cc:145] XLA service 0x5bd2cd672770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1725105195.432090      36 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1725105195.432094      36 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1725105216.222126      36 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Instruction:\nWhat should I do on a trip to Europe?\n\nResponse:\nYou should take a lot of money.\n\nWhat should you do to be a good student?\n\nResponse:\nYou should study hard.\n\nWhat should you do if you want to be rich?\n\nResponse:\nYou should get a job.\n\nWhat should you do when you are angry?\n\nResponse:\nYou should not hit anyone.\n\nWhat should you do if you are lost?\n\nResponse:\nYou should ask a local.\n\nWhat should you do if you are hungry?\n\nResponse:\nYou should eat something.\n\nWhat should you do if you are cold?\n\nResponse:\nYou should wear a jacket.\n\nWhat should you do if you want to be famous?\n\nResponse:\nYou should work hard.\n\nWhat should you do if you want to be healthy?\n\nResponse:\nYou should eat well.\n\nWhat should you do if you want to be popular?\n\nResponse:\nYou should be nice to people.\n\nWhat should you do if you want to be rich?\n\nResponse:\nYou should save money.\n\nWhat should you do if you want to be healthy?\n\nResponse:\nYou should take care of your body\n","output_type":"stream"}]},{"cell_type":"code","source":"gemma_lm.backbone.enable_lora(rank=4)\ngemma_lm.summary()","metadata":{"id":"T8mz71p0rr6G","colab":{"base_uri":"https://localhost:8080/","height":385},"outputId":"1d4d73e7-3036-4602-a511-f8a3d14efc8c","execution":{"iopub.status.busy":"2024-08-31T12:13:33.195862Z","iopub.execute_input":"2024-08-31T12:13:33.196279Z","iopub.status.idle":"2024-08-31T12:13:33.419391Z","shell.execute_reply.started":"2024-08-31T12:13:33.196241Z","shell.execute_reply":"2024-08-31T12:13:33.418426Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"gemma_lm.preprocessor.sequence_length = 256\n\noptimizer = keras.optimizers.AdamW(\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n)\n\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    optimizer=optimizer,\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"id":"zreUrcyiuFDb","execution":{"iopub.status.busy":"2024-08-31T12:13:36.714949Z","iopub.execute_input":"2024-08-31T12:13:36.715955Z","iopub.status.idle":"2024-08-31T12:13:36.734629Z","shell.execute_reply.started":"2024-08-31T12:13:36.715910Z","shell.execute_reply":"2024-08-31T12:13:36.733147Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"gemma_lm.fit(\n    data,\n    epochs=epochs,\n    batch_size=batch_size,\n)","metadata":{"id":"AcZmCvrHvA-X","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"id":"biAEq5ILxdqs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"What should I do on a trip to Europe?\",\n    response=\"\",\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"id":"CUoWfzocvSCR"},"execution_count":null,"outputs":[]}]}